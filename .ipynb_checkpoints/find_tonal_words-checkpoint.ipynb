{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XF7uamF6vyV"
   },
   "outputs": [],
   "source": [
    "import conllu, os, re, json\n",
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yOMQbH8RvKt"
   },
   "outputs": [],
   "source": [
    "def get_tonal_markup(wordlist1, bigram_list1, trigram_list1, wordlist2, bigram_list2, trigram_list2, aspects, conllu_folder):\n",
    "  new_path = conllu_folder + '_auto_processed'\n",
    "\n",
    "  if not os.path.exists(new_path):\n",
    "    os.mkdir(new_path)\n",
    "\n",
    "  for file in os.listdir(conllu_folder):\n",
    "    outp = []\n",
    "    path = os.path.join(conllu_folder, file)\n",
    "    t = conllu.parse(open(path, 'r', encoding='utf-8').read())\n",
    "    for sent_id, sent in enumerate(t):\n",
    "      sent_id += 1\n",
    "      t_id1 = 0\n",
    "      for token1, token2, token3 in zip([i['lemma'] for i in sent], [i['lemma'] for i in sent[1:]]+[''], [i['lemma'] for i in sent[2:]]+['','']):\n",
    "        t_id1 += 1\n",
    "        trigram = token1+' '+token2+' '+token3\n",
    "        bigram = token1+' '+token2\n",
    "        if trigram in trigram_list1:\n",
    "          outp.append(str(sent_id)+'\\t'+str(t_id1)+','+str(t_id1+2)+'\\t'+aspects[0]+'\\t'+str(trigram_list1[trigram]))\n",
    "        elif bigram in bigram_list1:\n",
    "          outp.append(str(sent_id)+'\\t'+str(t_id1)+','+str(t_id1+1)+'\\t'+aspects[0]+'\\t'+str(bigram_list1[bigram]))\n",
    "        elif token1 in wordlist1:\n",
    "          outp.append(str(sent_id)+'\\t'+str(t_id1)+'\\t'+aspects[0]+'\\t'+str(wordlist1[token1]))\n",
    "        elif trigram in trigram_list2:\n",
    "          outp.append(str(sent_id)+'\\t'+str(t_id1)+','+str(t_id1+2)+'\\t'+aspects[1]+'\\t'+str(trigram_list2[trigram]))\n",
    "        elif bigram in bigram_list2:\n",
    "          outp.append(str(sent_id)+'\\t'+str(t_id1)+','+str(t_id1+1)+'\\t'+aspects[1]+'\\t'+str(bigram_list2[bigram]))\n",
    "        elif token1 in wordlist2:\n",
    "          outp.append(str(sent_id)+'\\t'+str(t_id1)+'\\t'+aspects[1]+'\\t'+str(wordlist2[token1]))\n",
    "    \n",
    "    path = os.path.join(conllu_folder+\"_auto_processed\", file[:file.rfind('.')]+\"_auto_processed.tsv\")\n",
    "    with open(path, 'w', encoding='utf-8') as file_to_write:\n",
    "      for line in outp:\n",
    "        file_to_write.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ld_mcwS6vyY"
   },
   "outputs": [],
   "source": [
    "xtree = et.parse(\"development/SentiRuEval_rest_train.xml\")\n",
    "root = xtree.getroot()\n",
    "data = []\n",
    "columns = ['id', 'food', 'service', 'text']\n",
    "for review in root:\n",
    "    text_id = int(review.attrib['id'])\n",
    "    \n",
    "    scores = review.find('scores')\n",
    "    \n",
    "    food = int(scores.find('food').text)\n",
    "    service = int(scores.find('service').text)\n",
    "    \n",
    "    text = review.find('text').text\n",
    "    \n",
    "    data.append({'id': text_id,\n",
    "                'food': food,\n",
    "                'service': service,\n",
    "                'text': text})\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvSy9mkf6vya"
   },
   "outputs": [],
   "source": [
    "udpipe_model = Model.load('udpipe_models/russian-syntagrus-ud-2.0-170801.udpipe')\n",
    "pipeline = Pipeline(udpipe_model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0noJ8upS6vyb"
   },
   "outputs": [],
   "source": [
    "def udpipe_lemmatize(text, ud_pipeline=pipeline):\n",
    "    processed = conllu.parse(pipeline.process(text))\n",
    "    return [[token['lemma'] for token in sent if token['upostag']!='PUNCT'] for sent in processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBotglrELEyL"
   },
   "outputs": [],
   "source": [
    "t = conllu.parse(pipeline.process('Маша ела кашу, мама мыла раму'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kq89ecTz6vyd"
   },
   "outputs": [],
   "source": [
    "food_words = dict()\n",
    "service_words = dict()\n",
    "\n",
    "with open('development/Food_words.txt', 'r', encoding='utf-8') as inp:\n",
    "    for line in inp.readlines():\n",
    "        aspect, word, score = line.strip().split('        ')\n",
    "        food_words[word] = score\n",
    "\n",
    "with open('development/Service_words.txt', 'r', encoding='utf-8') as inp:\n",
    "    for line in inp.readlines():\n",
    "        aspect, word, score = line.strip().split('\\t')\n",
    "        service_words[word] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "9xIVOaAu6vyf",
    "outputId": "684e1564-9645-4015-e697-d415ff863d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943.tsv\n",
      "13823.tsv\n",
      "20086.tsv\n",
      "28083.tsv\n",
      "32840.tsv\n",
      "32856.tsv\n",
      "33591.tsv\n",
      "33693.tsv\n",
      "35486.tsv\n",
      "5648.tsv\n"
     ]
    }
   ],
   "source": [
    "new_food_words = dict()\n",
    "new_service_words = dict()\n",
    "\n",
    "for file in os.listdir('разметка_финал'):\n",
    "    if file.endswith('.tsv'):\n",
    "        print(file)\n",
    "        \n",
    "        with open(os.path.join('разметка_финал',file), 'r', encoding='utf-8') as inp:\n",
    "            lines = inp.readlines()\n",
    "\n",
    "        conllu_path = os.path.join('conllu_data', file)\n",
    "        with open(conllu_path, 'r', encoding='utf-8') as inp:\n",
    "            conll = conllu.parse(inp.read())\n",
    "\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                sent_id, token_ids, aspect, mark = line.strip().split('\\t')\n",
    "                mark = int(mark)\n",
    "                sent_id = int(sent_id)-1\n",
    "                token_ids = [int(i) for i in token_ids.split(',')]\n",
    "                start_id, end_id = int(token_ids[0])-1, int(token_ids[-1])\n",
    "                try:\n",
    "                    tokens = ' '.join(token['lemma'] for token in conll[sent_id][start_id:end_id])\n",
    "                except:\n",
    "                    print(sent_id, start_id, end_id)\n",
    "                if aspect.strip() == 'Service' and tokens not in new_service_words:\n",
    "                    new_service_words[tokens] = mark\n",
    "                elif aspect.strip() == 'Food' and tokens not in new_food_words:\n",
    "                    new_food_words[tokens] = mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylwon4fG6vyh"
   },
   "source": [
    "Посмотрим на пересечения выделенных нами словарей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "71QqBEzh6vyi",
    "outputId": "c839029a-b059-4f2d-cf8d-a790513e3a89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'большой',\n",
       "  'вкусный',\n",
       "  'невкусный',\n",
       "  'отличный',\n",
       "  'понравиться',\n",
       "  'прекрасный',\n",
       "  'сытный'},\n",
       " {'вежливый', 'ненавязчивый', 'приятный', 'хамоватый', 'хороший'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(food_words) & set(new_food_words), set(service_words) & set(new_service_words),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTLD8xTY6vyj"
   },
   "source": [
    "И на их объединение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hptblaGN6vyk",
    "outputId": "cfe9abe9-c770-45b7-a834-7cbc039c6147"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'10 балл',\n",
       "  'большой',\n",
       "  'великолепный',\n",
       "  'весь остыть',\n",
       "  'вкусно',\n",
       "  'вкусный',\n",
       "  'впечатлять',\n",
       "  'высокий все похвасть',\n",
       "  'горячий',\n",
       "  'достойный',\n",
       "  'единственный',\n",
       "  'интересный',\n",
       "  'не впечатлять',\n",
       "  'не очень дорого',\n",
       "  'невкусный',\n",
       "  'нежный',\n",
       "  'необычный',\n",
       "  'отличный',\n",
       "  'очень большой',\n",
       "  'очень вкусный',\n",
       "  'плохой',\n",
       "  'понравиться',\n",
       "  'посредственно',\n",
       "  'прекрасный',\n",
       "  'приятный',\n",
       "  'различный',\n",
       "  'разнообразный',\n",
       "  'разнообразный вкусный',\n",
       "  'расстроить',\n",
       "  'свежий',\n",
       "  'совершенно отвратительный',\n",
       "  'странный',\n",
       "  'сытный',\n",
       "  'хороший'},\n",
       " {'вежливый',\n",
       "  'веселый',\n",
       "  'внимательный',\n",
       "  'вполне приемлимый',\n",
       "  'высокий качество',\n",
       "  'гостеприимный',\n",
       "  'доброжелательный',\n",
       "  'дружелюбный',\n",
       "  'душевный',\n",
       "  'качественный',\n",
       "  'красивый',\n",
       "  'милый',\n",
       "  'не слишком вежливо',\n",
       "  'недолгий',\n",
       "  'ненавязчивый',\n",
       "  'оперативность',\n",
       "  'оперативный',\n",
       "  'отзывчивый',\n",
       "  'отличный',\n",
       "  'очень аккуратно',\n",
       "  'очень приветливый',\n",
       "  'плохо знать меню',\n",
       "  'понравиться',\n",
       "  'приветливый',\n",
       "  'приятный',\n",
       "  'радушный',\n",
       "  'с вызов',\n",
       "  'спасибо',\n",
       "  'трепетный',\n",
       "  'ужасный',\n",
       "  'улыбчивый',\n",
       "  'хамоватый',\n",
       "  'хороший',\n",
       "  'хорошо обучить'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(food_words)|set(new_food_words), set(service_words)|set(new_service_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRXLEcaU6vyl"
   },
   "source": [
    "Извлечём 1,2,3-граммы из development-корпуса - получим Unlabeled 1,2,3-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDhzmNER6vym"
   },
   "outputs": [],
   "source": [
    "def extract_123grams(texts, min_freq=5, process=udpipe_lemmatize):\n",
    "    ugram_freq_dict, bigram_freq_dict, trigram_freq_dict = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    for text in texts:\n",
    "        if type(text) == str and text:\n",
    "            try:\n",
    "              text = udpipe_lemmatize(text)\n",
    "            except:\n",
    "              print(text)\n",
    "              return None, None, None\n",
    "            ## extract 1grams\n",
    "            for sent in text:\n",
    "                for token1, token2, token3 in zip(sent, sent[1:], sent[2:]):\n",
    "                    ugram_freq_dict[token1] += 1\n",
    "                    bigram_freq_dict[token1+' '+token2] += 1\n",
    "                    trigram_freq_dict[token1+' '+token2+' '+token3] += 1\n",
    "                \n",
    "                if len(sent) > 0:\n",
    "                    ugram_freq_dict[sent[-1]] += 1\n",
    "                    if len(sent) > 1:\n",
    "                            bigram_freq_dict[sent[-2]+' '+sent[-1]] += 1\n",
    "                            ugram_freq_dict[sent[-2]] += 1\n",
    "            \n",
    "    ugram_freq_dict = {k:v for k,v in ugram_freq_dict.items() if v > min_freq}\n",
    "    bigram_freq_dict = {k:v for k,v in bigram_freq_dict.items() if v > min_freq}\n",
    "    trigram_freq_dict = {k:v for k,v in trigram_freq_dict.items() if v > min_freq}\n",
    "    \n",
    "    return ugram_freq_dict, bigram_freq_dict, trigram_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOxSu5Hk6vyn"
   },
   "outputs": [],
   "source": [
    "ugrams,bigrams,trigrams  = extract_123grams(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lLfX7qQNig87",
    "outputId": "974f2bad-8f72-4e3e-b153-e5290ab14b3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17823, 57040, 24075)"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ugrams), len(bigrams), len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHBseXdkj-ol"
   },
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "6bTDOP_ckC1A",
    "outputId": "cad23e64-3f62-443b-808c-7a89e0c01c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoM_efnXkdV1"
   },
   "outputs": [],
   "source": [
    "os.chdir('drive/My Drive/АвтОбрЕЯ SuperPowerTeam3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imSmKg7Ok67G"
   },
   "outputs": [],
   "source": [
    "os.mkdir('Частотные словари development корпус')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBh-6tGUlfl2"
   },
   "outputs": [],
   "source": [
    "os.chdir('Частотные словари development корпус')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvnWzBlPlzJk"
   },
   "outputs": [],
   "source": [
    "with open('ugrams.json', 'w', encoding='utf-8') as ugram_outp:\n",
    "  json.dump(ugrams, ugram_outp, ensure_ascii=False)\n",
    "\n",
    "with open('bigrams.json', 'w', encoding='utf-8') as bigram_outp:\n",
    "  json.dump(bigrams, bigram_outp, ensure_ascii=False)\n",
    "\n",
    "with open('trigrams.json', 'w', encoding='utf-8') as trigram_outp:\n",
    "  json.dump(trigrams, trigram_outp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAd0kkHlo6_G"
   },
   "outputs": [],
   "source": [
    "all_food = dict()\n",
    "all_service = dict()\n",
    "\n",
    "for gram in set(food_words)|set(new_food_words):\n",
    "  if gram in food_words:\n",
    "    all_food[gram] = food_words[gram]\n",
    "  elif gram in new_food_words:\n",
    "    all_food[gram] = new_food_words[gram]\n",
    "\n",
    "for gram in set(service_words)|set(new_service_words):\n",
    "  if gram in service_words:\n",
    "    all_service[gram] = service_words[gram]\n",
    "  elif gram in new_service_words:\n",
    "    all_service[gram] = new_service_words[gram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pABCnZaqNeQ"
   },
   "outputs": [],
   "source": [
    "food_unigrams = {k:v for k,v in all_food.items() if k.count(' ')==0}\n",
    "food_bigrams = {k:v for k,v in all_food.items() if k.count(' ')==1}\n",
    "food_trigrams = {k:v for k,v in all_food.items() if k.count(' ')==2}\n",
    "\n",
    "service_unigrams = {k:v for k,v in all_service.items() if k.count(' ')==0}\n",
    "service_bigrams = {k:v for k,v in all_service.items() if k.count(' ')==1}\n",
    "service_trigrams = {k:v for k,v in all_service.items() if k.count(' ')==2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgnniXX5vf-j"
   },
   "outputs": [],
   "source": [
    "def extract_123grams(texts, min_freq=5, conllu_folder=False):\n",
    "    ugram_freq_dict, bigram_freq_dict, trigram_freq_dict = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    if conllu_folder:\n",
    "        texts = [conllu.parse(open(os.path.join(texts,i), 'r', encoding='utf-8').read())for i in os.listdir(texts)]\n",
    "        for text_id, text in enumerate(texts):\n",
    "          for sent_id, sent in enumerate(text):\n",
    "            texts[text_id][sent_id] = [i['lemma'] for i in sent if i['upostag']!='PUNCT']\n",
    "    for text in texts:\n",
    "        if not conllu_folder:\n",
    "            if type(text) == str and text:\n",
    "                try:\n",
    "                  text = udpipe_lemmatize(text)\n",
    "                except:\n",
    "                  print(text)\n",
    "                  return None, None, None\n",
    "            else:\n",
    "                continue\n",
    "            ## extract 1grams\n",
    "        for sent in text:\n",
    "            for token1, token2, token3 in zip(sent, sent[1:], sent[2:]):\n",
    "                ugram_freq_dict[token1] += 1\n",
    "                bigram_freq_dict[token1+' '+token2] += 1\n",
    "                trigram_freq_dict[token1+' '+token2+' '+token3] += 1\n",
    "            \n",
    "            if len(sent) > 0:\n",
    "                ugram_freq_dict[sent[-1]] += 1\n",
    "                if len(sent) > 1:\n",
    "                    bigram_freq_dict[sent[-2]+' '+sent[-1]] += 1\n",
    "                    ugram_freq_dict[sent[-2]] += 1\n",
    "            \n",
    "    ugram_freq_dict = {k:v for k,v in ugram_freq_dict.items() if v > min_freq}\n",
    "    bigram_freq_dict = {k:v for k,v in bigram_freq_dict.items() if v > min_freq}\n",
    "    trigram_freq_dict = {k:v for k,v in trigram_freq_dict.items() if v > min_freq}\n",
    "    \n",
    "    return ugram_freq_dict, bigram_freq_dict, trigram_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HN8QK9cjsVR_"
   },
   "outputs": [],
   "source": [
    "train_unigrams, train_bigrams, train_trigrams = extract_123grams('conllu_data', conllu_folder=True, min_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRP-TCFQxDEv"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VmYxNnfxdaJ"
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(tokenizer=lambda x: [j for i in udpipe_lemmatize(x) for j in i]).fit([i for i in df['text'] if type(i)==str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27nD3PDiJFtX"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8625e040eb61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0midf_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0midf_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vec' is not defined"
     ]
    }
   ],
   "source": [
    "idf_vocab = dict()\n",
    "\n",
    "for word in vec.vocabulary_:\n",
    "  idf_vocab[word] = vec.idf_[vec.vocabulary_[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSIB6kvRMeky"
   },
   "outputs": [],
   "source": [
    "with open('idf_vocab.json', 'w', encoding='utf-8') as outp:\n",
    "  json.dump(idf_vocab, outp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYrfA-DnsoKv"
   },
   "outputs": [],
   "source": [
    "tonal_df = pd.read_excel(\"linis_dictionary/words_all_full_rating.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "sLT8OhdSOiXt",
    "outputId": "75556858-167b-4b27-f6da-eb9b87082242"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>mean</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>average rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аборигенный</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>аборт</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>абрамович</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>абсолютный</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>абстрактный</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.874890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7540</th>\n",
       "      <td>ярый</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.942809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>ясно</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>ясность</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>ясный</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>ячейка</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7545 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Words      mean  dispersion  average rate\n",
       "0     аборигенный -0.250000    0.433013             0\n",
       "1           аборт -1.000000    0.816497            -1\n",
       "2       абрамович  0.000000    0.000000             0\n",
       "3      абсолютный  0.333333    0.471405             0\n",
       "4     абстрактный -0.111111    0.874890             0\n",
       "...           ...       ...         ...           ...\n",
       "7540         ярый -0.333333    0.942809             0\n",
       "7541         ясно  0.000000    0.000000             0\n",
       "7542      ясность  0.666667    0.471405             1\n",
       "7543        ясный  0.666667    0.471405             1\n",
       "7544       ячейка  0.000000    0.000000             0\n",
       "\n",
       "[7545 rows x 4 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tonal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GY0oTv0bPENl"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "HF3VAn00Qwc_",
    "outputId": "2b413ba7-db10-4e91-8d97-5d767bb2f619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-23 19:39:05--  http://vectors.nlpl.eu/repository/11/182.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 637613765 (608M) [application/zip]\n",
      "Saving to: ‘182.zip’\n",
      "\n",
      "182.zip               4%[                    ]  27.40M   162KB/s    eta 27m 24s^C\n"
     ]
    }
   ],
   "source": [
    "!wget http://vectors.nlpl.eu/repository/11/182.zip"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "find tonal words.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
