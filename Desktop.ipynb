{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import keras.backend\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, transforms\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtree = et.parse(urllib.request.urlopen(\"https://raw.githubusercontent.com/nicklogin/NLPTeam3000/master/development/SentiRuEval_rest_train.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/nicklogin/NLPTeam3000/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip master.zip.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = xtree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "columns = ['id', 'food', 'service', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in root:\n",
    "    text_id = int(review.attrib['id'])\n",
    "    \n",
    "    scores = review.find('scores')\n",
    "    \n",
    "    food = int(scores.find('food').text)\n",
    "    service = int(scores.find('service').text)\n",
    "    \n",
    "    text = review.find('text').text\n",
    "\n",
    "    new_food = 0\n",
    "    new_service = 0\n",
    "\n",
    "    if food > 5:\n",
    "        new_food = 1\n",
    "\n",
    "    if service > 5:\n",
    "        new_service = 1\n",
    "    \n",
    "    data.append({'id': text_id,\n",
    "                'food': new_food,\n",
    "                'service': new_service,\n",
    "                'text': text})\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'И пускай на меня не обижается наш прославленный защитник - франкофон «Монреаль Канадиенс» Maxime – я всегда с некоторой опаской относился к этому народу. Народу способному с таким благоговением доводить до цирроза всю пернатую живность, заставлять специальных поисковых свиней копошиться в грязи в поисках сумчатых грибов, ковыряться в тине, собирая брюхоногих и двустворчатых. Народу, подсадившего все население земного шара на муть, именуемое «Божоле Нуво», на сыры с плесенью и на квакающих жаб, скачущих по болотам. Тем не менее, единожды заглянув, не мог успокоиться, пока не вкусил большую часть меню. Не буду вдаваться в детали «вкусно-невкусно», а приведу турнирную таблицу, коли начали с хоккея. 1.Петух, запеченный в сливках, с картофелем и грибами. Почему петух, а не курица? Различать еще не научился. 2.Домашний пирог. Яблочный. Горячий. Тающий. 3.Говядина по-бургунски. Кускус неожиданный. 4.Террин из печени цыпленка. Массивная порция. 5.Утиная ножка «Конфи» с картофельным пюре. Мягко 6.Пенне с артишоками. Много чеснока. Я люблю, но если дышать потом на кого-нибудь, может случиться казус. 7.Ростбиф с маринованным луком. 8.Лягушка. На вкус смесь птицы и рыбы. Овощное сате на гарнир. 9.Улитки «Эскарго». Просто в масле мне нравятся больше. 10.Гребешки со спаржей. Горячее блюдо. Весьма диетично. 11.Гребешки с грушей. Горячая закуска. Рядом просто листья салата. 12.Теплый салат с говядиной. Рядом просто листья салата. 13.Тирамису. Качественный уровень соблюден. 14.Домашний пирог. Черничный. Холодный. Слишком холодный творог в основном. Плюс, другие приятности. Приятный грейпфрутовый «фреш», приятный зеленый чай «Земляника со сливками», приятная тертая клубника комплиментом – всегда и приятный глинтвейн – не всегда. Один раз зажали. По интерьеру комментария два. А нельзя было по краям кабинки-диванчики сварганить для уюта? Одного милого столика справа от входа маловато. Помывочная с белыми дверями. Почему? Выбивается из стиля. В остальном мой типаж. Музыка мягкая. Панели нет! Сервис с улыбкой. Шероховатости в пределах нормы и незначительны. Один раз приборы забыли принести, ну и глинтвейн зажали, это я уже говорил. Человек к хорошему быстро привыкает…'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root[0].find('text').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txts = []\n",
    "food_negs = []\n",
    "food_poss = []\n",
    "service_negs = []\n",
    "service_poss = []\n",
    "\n",
    "\n",
    "\n",
    "for file in os.listdir(\"Downloads/NLPTeam3000-master-y/conllu_data\"):\n",
    "    if file.endswith('.tsv'):\n",
    "        file_text = open(os.path.join(\"Downloads/NLPTeam3000-master-y/conllu_data\", file), 'r', encoding='utf-8').read()\n",
    "        text = ''\n",
    "        sentences = []\n",
    "        for l in file_text.splitlines():\n",
    "            if '# text = ' in l:\n",
    "                sentences.append(l.replace('# text = ', ''))\n",
    "                text += l.replace('# text = ', '')\n",
    "    test_txts.append(text)\n",
    "\n",
    "\n",
    "    food_neg = []\n",
    "    food_pos = []\n",
    "    service_neg = []\n",
    "    service_pos = []\n",
    "\n",
    "\n",
    "    markup = open(os.path.join('Downloads/NLPTeam3000-master-y/разметка_финал', file[:-4]+\".tsv\"), 'r', encoding='utf-8').read()\n",
    "    for line in markup.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            l = line.split('\\t')[0]\n",
    "            ws = []\n",
    "            for n in line.split('\\t')[1].split(','):\n",
    "                ws.append(sentences[int(l) - 1][int(n) - 1])\n",
    "\n",
    "            if line.split('\\t')[2] == 'Food' and int(line.split('\\t')[3]) == 0:\n",
    "                food_neg.append(ws)\n",
    "            elif line.split('\\t')[2] == 'Food' and int(line.split('\\t')[3]) == 1:\n",
    "                food_pos.append(ws)\n",
    "            elif line.split('\\t')[2] == 'Service' and int(line.split('\\t')[3]) == 0:\n",
    "                service_neg.append(ws)\n",
    "            elif line.split('\\t')[2] == 'Service' and int(line.split('\\t')[3]) == 1:\n",
    "                service_pos.append(ws)\n",
    "\n",
    "    food_negs.append(food_neg)\n",
    "    food_poss.append(food_pos)\n",
    "    service_negs.append(service_neg)\n",
    "    service_poss.append(service_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(test_txts))\n",
    "print(len(food_negs))\n",
    "print(len(food_poss))\n",
    "print(len(service_negs))\n",
    "print(len(service_poss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "foods = []\n",
    "services = []\n",
    "\n",
    "for review in root:\n",
    "    \n",
    "    scores = review.find('scores')\n",
    "    \n",
    "    food = int(scores.find('food').text)\n",
    "    service = int(scores.find('service').text)\n",
    "    \n",
    "    texts.append(review.find('text').text)\n",
    "\n",
    "    new_food = 0\n",
    "    new_service = 0\n",
    "\n",
    "    if food > 5:\n",
    "        new_food = 1\n",
    "\n",
    "    if service > 5:\n",
    "        new_service = 1\n",
    "    \n",
    "    foods.append(new_food)\n",
    "    services.append(new_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19034\n",
      "19034\n",
      "19034\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(len(foods))\n",
    "print(len(services))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://vectors.nlpl.eu/repository/11/180.zip\n",
    "#!unzip 180.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(texts):\n",
    "    return max(len(t) for t in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_modelf = KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "w2v_models = KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "scores_trainf, scores_valf, texts_trainf, texts_valf = train_test_split(\n",
    "    foods[:18943], texts[:18943], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "scores_trains, scores_vals, texts_trains, texts_vals = train_test_split(\n",
    "    services[:18943], texts[:18943], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "MAX_LENf = max(max_length(texts_trainf), max_length(texts_valf))\n",
    "MAX_LENs = max(max_length(texts_trains), max_length(texts_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasetf(lines, embedding_dim, num_examples=None):\n",
    "    prep = lines[:num_examples]\n",
    "    vocab = Counter()\n",
    "    x_tensor = np.zeros((len(prep), MAX_LENf, embedding_dim))\n",
    "    for i, text in enumerate(prep):\n",
    "        for j, w in enumerate(text):\n",
    "            try:\n",
    "                x_tensor[i, j, :] = w2v_modelf[w]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vocab[w] += 1\n",
    "    return x_tensor, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(lines, embedding_dim, num_examples=None):\n",
    "    prep = lines[:num_examples]\n",
    "    vocab = Counter()\n",
    "    x_tensor = np.zeros((len(prep), MAX_LENs, embedding_dim))\n",
    "    for i, text in enumerate(prep):\n",
    "        for j, w in enumerate(text):\n",
    "            try:\n",
    "                x_tensor[i, j, :] = w2v_models[w]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vocab[w] += 1\n",
    "    return x_tensor, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_trainf, inp_vocab_trainf = load_datasetf(texts_trainf, w2v_modelf.vector_size)\n",
    "input_tensor_valf, inp_vocab_valf = load_datasetf(texts_valf, w2v_modelf.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_trains, inp_vocab_trains = load_datasets(texts_trains, w2v_models.vector_size)\n",
    "input_tensor_vals, inp_vocab_vals = load_datasets(texts_vals, w2v_models.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelf.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_models.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13260, 23438, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_trainf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13260, 23438, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_trains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimf = w2v_modelf.vector_size\n",
    "inp_vocabf = inp_vocab_trainf + inp_vocab_valf\n",
    "vocab_inp_sizef = len(inp_vocabf)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = w2v_models.vector_size\n",
    "inp_vocabs = inp_vocab_trains + inp_vocab_vals\n",
    "vocab_inp_sizes = len(inp_vocabs)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from innvestigate.utils.tests.networks import base as network_base\n",
    "def build_network(max_len, voc_size, embedding_dim, output_n, activation=None, dense_unit=256, dropout_rate=0.25):\n",
    "    if activation:\n",
    "        activation = \"relu\"\n",
    "\n",
    "    net = {}\n",
    "    net[\"in\"] = keras.Input(shape=[1, max_len, embedding_dim])\n",
    "    net[\"conv\"] = keras.layers.Conv2D(filters=100, kernel_size=(1,2), strides=(1, 1), padding='valid')(net[\"in\"])\n",
    "    net[\"pool\"] = keras.layers.MaxPooling2D(pool_size=(1, max_len-1), strides=(1,1))(net[\"conv\"])\n",
    "    net[\"out\"] = network_base.dense_layer(keras.layers.Flatten()(net[\"pool\"]), units=output_n, activation=activation)\n",
    "    net[\"sm_out\"] = network_base.softmax(net[\"out\"])\n",
    "\n",
    "\n",
    "    net.update({\n",
    "        \"input_shape\": [1, max_len, embedding_dim],\n",
    "        \"output_n\": output_n,\n",
    "    })\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1224 23:55:32.361495 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1224 23:55:32.389413 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1224 23:55:32.397359 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1224 23:55:32.423907 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netf = build_network(MAX_LENf, vocab_inp_sizef, embedding_dimf, 2)\n",
    "model_without_softmaxf = keras.models.Model(inputs=netf['in'], outputs=netf['out'])\n",
    "model_with_softmaxf = keras.models.Model(inputs=netf['in'], outputs=netf['sm_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = build_network(MAX_LENs, vocab_inp_sizes, embedding_dims, 2)\n",
    "model_without_softmaxs = keras.models.Model(inputs=nets['in'], outputs=nets['out'])\n",
    "model_with_softmaxs = keras.models.Model(inputs=nets['in'], outputs=nets['sm_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 23438, 300)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 23437, 100)     60100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 60,302\n",
      "Trainable params: 60,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_without_softmaxs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 23438, 300)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 23437, 100)     60100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 60,302\n",
      "Trainable params: 60,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_without_softmaxf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    return keras.utils.to_categorical(y, 2)\n",
    "\n",
    "def train_modelf(model, epochs=20):\n",
    "    \n",
    "    x_train = np.expand_dims(input_tensor_trainf, axis=1)\n",
    "    y_train = to_one_hot(scores_trainf)\n",
    "    \n",
    "    x_val = np.expand_dims(input_tensor_valf, axis=1)\n",
    "    y_val = to_one_hot(scores_valf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=25,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        shuffle=True\n",
    "                       )\n",
    "\n",
    "def train_models(model, epochs=20):\n",
    "    \n",
    "    x_train = np.expand_dims(input_tensor_trains, axis=1)\n",
    "    y_train = to_one_hot(scores_trains)\n",
    "    \n",
    "    x_val = np.expand_dims(input_tensor_vals, axis=1)\n",
    "    y_val = to_one_hot(scores_vals)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=25,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        shuffle=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1224 23:55:32.640120 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1224 23:55:32.655418 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1224 23:55:32.803020 4642790848 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1224 23:55:32.869693 4642790848 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13260 samples, validate on 5683 samples\n",
      "Epoch 1/3\n",
      " 1100/13260 [=>............................] - ETA: 52:01 - loss: 0.6137 - acc: 0.7400"
     ]
    }
   ],
   "source": [
    "train_modelf(model_with_softmaxf, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models(model_with_softmaxs, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_softmaxf.set_weights(model_with_softmaxf.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_softmaxs.set_weights(model_with_softmaxs.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['gradient', 'lrp.z', 'lrp.alpha_2_beta_1', 'pattern.attribution']\n",
    "kwargs = [{}, {}, {}, {'pattern_type': 'relu'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import investigate\n",
    "analyzersf = []\n",
    "\n",
    "for method, kws in zip(methods, kwargs):\n",
    "    analyzerf = innvestigate.create_analyzer(method, model_without_softmaxf, **kws)\n",
    "    analyzerf.fit(np.expand_dims(input_tensor_trainf, axis=1), batch_size=256, verbose=1)\n",
    "    analyzersf.append(analyzerf)\n",
    "    \n",
    "analyzerss = []\n",
    "\n",
    "for method, kws in zip(methods, kwargs):\n",
    "    analyzers = innvestigate.create_analyzer(method, model_without_softmaxs, **kws)\n",
    "    analyzers.fit(np.expand_dims(input_tensor_trains, axis=1), batch_size=256, verbose=1)\n",
    "    analyzerss.append(analyzers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scoresf(X, Y, ridx):\n",
    "    max_len = max_length(input_tensor_trainf)\n",
    "\n",
    "    analysis = np.zeros([len(analyzers), 1, max_len])\n",
    "    x, y = X[ridx], Y[ridx]\n",
    "    t_start = time.time()\n",
    "    x = x.reshape((1, 1, max_len, embedding_dimf))\n",
    "    presm = model_without_softmaxf.predict_on_batch(x)[0] #forward pass without softmax\n",
    "    prob = model_with_softmaxf.predict_on_batch(x)[0] #forward pass with softmax\n",
    "    y_hat = prob.argmax()\n",
    "  \n",
    "    for aidx, analyzer in enumerate(analyzersf):\n",
    "        a = np.squeeze(analyzer.analyze(x))\n",
    "        a = np.sum(a, axis=1)\n",
    "        analysis[aidx] = a\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print('Review %d (%.4fs)'% (ridx, t_elapsed))\n",
    "    return analysis, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scoress(X, Y, ridx):\n",
    "    max_len = max_length(input_tensor_trains)\n",
    "\n",
    "    analysis = np.zeros([len(analyzers), 1, max_len])\n",
    "    x, y = X[ridx], Y[ridx]\n",
    "    t_start = time.time()\n",
    "    x = x.reshape((1, 1, max_len, embedding_dims))\n",
    "    presm = model_without_softmaxs.predict_on_batch(x)[0] #forward pass without softmax\n",
    "    prob = model_with_softmaxs.predict_on_batch(x)[0] #forward pass with softmax\n",
    "    y_hat = prob.argmax()\n",
    "  \n",
    "    for aidx, analyzer in enumerate(analyzerss):\n",
    "        a = np.squeeze(analyzer.analyze(x))\n",
    "        a = np.sum(a, axis=1)\n",
    "        analysis[aidx] = a\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print('Review %d (%.4fs)'% (ridx, t_elapsed))\n",
    "    return analysis, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_scoresf(input_tensor_trainf, scores_trainf, 97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_scoress(input_tensor_trains, scores_trains, 97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_heatmap(words, scores, title=\"\", width=5, height=0.2, verbose=0, max_word_per_line=10):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    \n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_title(title, loc='left')\n",
    "    tokens = words\n",
    "    if verbose > 0:\n",
    "        print('len words : %d | len scores : %d' % (len(words), len(scores)))\n",
    "\n",
    "    cmap = plt.cm.ScalarMappable(cmap=cm.bwr)\n",
    "    cmap.set_clim(0, 1)\n",
    "    \n",
    "    canvas = ax.figure.canvas\n",
    "    t = ax.transData\n",
    "\n",
    "    # normalize scores to the followings:\n",
    "    # - negative scores in [0, 0.5]\n",
    "    # - positive scores in (0.5, 1]\n",
    "    normalized_scores = 0.5 * scores / np.max(np.abs(scores)) + 0.5\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print('Raw score')\n",
    "        print(scores)\n",
    "        print('Normalized score')\n",
    "        print(normalized_scores)\n",
    "\n",
    "    # make sure the heatmap doesn't overlap with the title\n",
    "    loc_y = -0.2\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        *rgb, _ = cmap.to_rgba(normalized_scores[i], bytes=True)\n",
    "        color = '#%02x%02x%02x' % tuple(rgb)\n",
    "        \n",
    "        text = ax.text(0.0, loc_y, token,\n",
    "                       bbox={\n",
    "                           'facecolor': color,\n",
    "                           'pad': 5.0,\n",
    "                           'linewidth': 1,\n",
    "                           'boxstyle': 'round,pad=0.5'\n",
    "                       }, transform=t)\n",
    "\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "        \n",
    "        # create a new line if the line exceeds the length\n",
    "        if (i+1) % max_word_per_line == 0:\n",
    "            loc_y = loc_y -  2.5\n",
    "            t = ax.transData\n",
    "        else:\n",
    "            t = transforms.offset_copy(text._transform, x=ex.width+15, units='dots')\n",
    "\n",
    "    if verbose == 0:\n",
    "        ax.axis('off')def plot_text_heatmap(words, scores, title=\"\", width=5, height=0.2, verbose=0, max_word_per_line=10):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    \n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_title(title, loc='left')\n",
    "    tokens = words\n",
    "    if verbose > 0:\n",
    "        print('len words : %d | len scores : %d' % (len(words), len(scores)))\n",
    "\n",
    "    cmap = plt.cm.ScalarMappable(cmap=cm.bwr)\n",
    "    cmap.set_clim(0, 1)\n",
    "    \n",
    "    canvas = ax.figure.canvas\n",
    "    t = ax.transData\n",
    "\n",
    "    # normalize scores to the followings:\n",
    "    # - negative scores in [0, 0.5]\n",
    "    # - positive scores in (0.5, 1]\n",
    "    normalized_scores = 0.5 * scores / np.max(np.abs(scores)) + 0.5\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print('Raw score')\n",
    "        print(scores)\n",
    "        print('Normalized score')\n",
    "        print(normalized_scores)\n",
    "\n",
    "    # make sure the heatmap doesn't overlap with the title\n",
    "    loc_y = -0.2\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        *rgb, _ = cmap.to_rgba(normalized_scores[i], bytes=True)\n",
    "        color = '#%02x%02x%02x' % tuple(rgb)\n",
    "        \n",
    "        text = ax.text(0.0, loc_y, token,\n",
    "                       bbox={\n",
    "                           'facecolor': color,\n",
    "                           'pad': 5.0,\n",
    "                           'linewidth': 1,\n",
    "                           'boxstyle': 'round,pad=0.5'\n",
    "                       }, transform=t)\n",
    "\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "        \n",
    "        # create a new line if the line exceeds the length\n",
    "        if (i+1) % max_word_per_line == 0:\n",
    "            loc_y = loc_y -  2.5\n",
    "            t = ax.transData\n",
    "        else:\n",
    "            t = transforms.offset_copy(text._transform, x=ex.width+15, units='dots')\n",
    "\n",
    "    if verbose == 0:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af, y_predf = analyze_scoresf(input_tensor_trainf, scores_trainf, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass, y_preds = analyze_scoress(input_tensor_trains, scores_trains, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'af' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-523d28c58a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'af' is not defined"
     ]
    }
   ],
   "source": [
    "af[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_heatmap(\n",
    "    texts_train[100],\n",
    "    af[0][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_heatmap(\n",
    "    texts_train[100],\n",
    "    ass[0][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "wordsf = texts_valf[idx]\n",
    "wordss = texts_vals[idx]\n",
    "    \n",
    "print('Review(id=%d): %s' % (idx, ' '.join(wordsf)))\n",
    "y_truef = scores_valf[idx]\n",
    "af, y_predf = analyze_scoresf(input_tensor_valf, scores_valf, idx)\n",
    "\n",
    "print(\"Pred class : %d %s\" %\n",
    "      (y_predf, '✓' if y_predf == y_truef else '✗ (%d)' % y_truef)\n",
    "      )\n",
    "                            \n",
    "for j, method in enumerate(methods):\n",
    "    plot_text_heatmap(wordsf, af[j].reshape(-1), title='Method: %s' % method, verbose=0)\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "print('Review(id=%d): %s' % (idx, ' '.join(wordss)))\n",
    "y_trues = scores_vals[idx]\n",
    "ass, y_preds = analyze_scoress(input_tensor_vals, scores_vals, idx)\n",
    "\n",
    "print(\"Pred class : %d %s\" %\n",
    "      (y_preds, '✓' if y_preds == y_trues else '✗ (%d)' % y_trues)\n",
    "      )\n",
    "\n",
    "for j, method in enumerate(methods):\n",
    "    plot_text_heatmap(wordss, ass[j].reshape(-1), title='Method: %s' % method, verbose=0)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
