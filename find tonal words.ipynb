{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu, os\n",
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtree = et.parse(\"development/SentiRuEval_rest_train.xml\")\n",
    "root = xtree.getroot()\n",
    "data = []\n",
    "columns = ['id', 'food', 'service', 'text']\n",
    "for review in root:\n",
    "    text_id = int(review.attrib['id'])\n",
    "    \n",
    "    scores = review.find('scores')\n",
    "    \n",
    "    food = int(scores.find('food').text)\n",
    "    service = int(scores.find('service').text)\n",
    "    \n",
    "    text = review.find('text').text\n",
    "    \n",
    "    data.append({'id': text_id,\n",
    "                'food': food,\n",
    "                'service': service,\n",
    "                'text': text})\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpipe_model = Model.load('udpipe_models/russian-syntagrus-ud-2.0-170801.udpipe')\n",
    "pipeline = Pipeline(udpipe_model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpipe_lemmatize(text, ud_pipeline=pipeline):\n",
    "    processed = conllu.parse(pipeline.process(text))\n",
    "    return [[token['lemma'] for token in sent] for sent in processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_words = dict()\n",
    "service_words = dict()\n",
    "\n",
    "with open('development/Food_words.txt', 'r', encoding='utf-8') as inp:\n",
    "    for line in inp.readlines():\n",
    "        aspect, word, score = line.strip().split('        ')\n",
    "        food_words[word] = score\n",
    "\n",
    "with open('development/Service_words.txt', 'r', encoding='utf-8') as inp:\n",
    "    for line in inp.readlines():\n",
    "        aspect, word, score = line.strip().split('\\t')\n",
    "        service_words[word] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943.tsv\n",
      "13823.tsv\n",
      "20086.tsv\n",
      "28083.tsv\n",
      "32840.tsv\n",
      "32856.tsv\n",
      "33591.tsv\n",
      "33693.tsv\n",
      "35486.tsv\n",
      "5648.tsv\n"
     ]
    }
   ],
   "source": [
    "new_food_words = dict()\n",
    "new_service_words = dict()\n",
    "\n",
    "for file in os.listdir('разметка_финал'):\n",
    "    if file.endswith('.tsv'):\n",
    "        print(file)\n",
    "        \n",
    "        with open(os.path.join('разметка_финал',file), 'r', encoding='utf-8') as inp:\n",
    "            lines = inp.readlines()\n",
    "\n",
    "        conllu_path = os.path.join('conllu_data', file)\n",
    "        with open(conllu_path, 'r', encoding='utf-8') as inp:\n",
    "            conll = conllu.parse(inp.read())\n",
    "\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                sent_id, token_ids, aspect, mark = line.strip().split('\\t')\n",
    "                mark = int(mark)\n",
    "                sent_id = int(sent_id)-1\n",
    "                token_ids = [int(i) for i in token_ids.split(',')]\n",
    "                start_id, end_id = int(token_ids[0])-1, int(token_ids[-1])\n",
    "                try:\n",
    "                    tokens = ' '.join(token['lemma'] for token in conll[sent_id][start_id:end_id])\n",
    "                except:\n",
    "                    print(sent_id, start_id, end_id)\n",
    "                if aspect.strip() == 'Service' and tokens not in new_service_words:\n",
    "                    new_service_words[tokens] = mark\n",
    "                elif aspect.strip() == 'Food' and tokens not in new_food_words:\n",
    "                    new_food_words[tokens] = mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на пересечения выделенных нами словарей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'большой',\n",
       "  'вкусный',\n",
       "  'невкусный',\n",
       "  'отличный',\n",
       "  'понравиться',\n",
       "  'прекрасный',\n",
       "  'сытный'},\n",
       " {'вежливый', 'ненавязчивый', 'приятный', 'хамоватый', 'хороший'})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(food_words) & set(new_food_words), set(service_words) & set(new_service_words),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И на их объединение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'10 балл',\n",
       "  'большой',\n",
       "  'великолепный',\n",
       "  'весь остыть',\n",
       "  'вкусно',\n",
       "  'вкусный',\n",
       "  'впечатлять',\n",
       "  'высокий все похвасть',\n",
       "  'горячий',\n",
       "  'достойный',\n",
       "  'единственный',\n",
       "  'интересный',\n",
       "  'не впечатлять',\n",
       "  'не очень дорого',\n",
       "  'невкусный',\n",
       "  'нежный',\n",
       "  'необычный',\n",
       "  'отличный',\n",
       "  'очень большой',\n",
       "  'очень вкусный',\n",
       "  'плохой',\n",
       "  'понравиться',\n",
       "  'посредственно',\n",
       "  'прекрасный',\n",
       "  'приятный',\n",
       "  'различный',\n",
       "  'разнообразный',\n",
       "  'разнообразный вкусный',\n",
       "  'расстроить',\n",
       "  'свежий',\n",
       "  'совершенно отвратительный',\n",
       "  'странный',\n",
       "  'сытный',\n",
       "  'хороший'},\n",
       " {'вежливый',\n",
       "  'веселый',\n",
       "  'внимательный',\n",
       "  'вполне приемлимый',\n",
       "  'высокий качество',\n",
       "  'гостеприимный',\n",
       "  'доброжелательный',\n",
       "  'дружелюбный',\n",
       "  'душевный',\n",
       "  'качественный',\n",
       "  'красивый',\n",
       "  'милый',\n",
       "  'не слишком вежливо',\n",
       "  'недолгий',\n",
       "  'ненавязчивый',\n",
       "  'оперативность',\n",
       "  'оперативный',\n",
       "  'отзывчивый',\n",
       "  'отличный',\n",
       "  'очень аккуратно',\n",
       "  'очень приветливый',\n",
       "  'плохо знать меню',\n",
       "  'понравиться',\n",
       "  'приветливый',\n",
       "  'приятный',\n",
       "  'радушный',\n",
       "  'с вызов',\n",
       "  'спасибо',\n",
       "  'трепетный',\n",
       "  'ужасный',\n",
       "  'улыбчивый',\n",
       "  'хамоватый',\n",
       "  'хороший',\n",
       "  'хорошо обучить'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(food_words)|set(new_food_words), set(service_words)|set(new_service_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечём 1,2,3-граммы из development-корпуса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_123grams(texts, min_freq=5):\n",
    "    ugram_freq_dict, bigram_freq_dict, trigram_freq_dict = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    print(\"Lemmatizing with udpipe\")\n",
    "    texts = udpipe_lemmatize('\\n\\n'.join(texts))\n",
    "    print(\"Extracting 1,2,3 grams\")\n",
    "    ## extract 1grams\n",
    "    for sent in texts:\n",
    "        for token1, token2, token3 in zip(sent, sent[1:], sent[2:]):\n",
    "            ugram_freq_dict[token1] += 1\n",
    "            bigram_freq_dict[token1+' '+token] += 1\n",
    "            trigram_freq_dict[token1+' '+token2+' '+token3] += 1\n",
    "        \n",
    "        if len(sent) > 0:\n",
    "            ugram_freq_dict[sent[-1]] += 1\n",
    "            if len(sent) > 1:\n",
    "                bigram_freq_dict[sent[-2]+' '+sent[-1]] += 1\n",
    "                ugram_freq_dict[sent[-2]] += 1\n",
    "    \n",
    "    ugram_freq_dict = {k:v for k,v in ugram_freq_dict.items() if v > min_freq}\n",
    "    bigram_freq_dict = {k:v for k,v in bigram_freq_dict.items() if v > min_freq}\n",
    "    trigram_freq_dict = {k:v for k,v in trigram_freq_dict.items() if v > min_freq}\n",
    "    \n",
    "    return ugram_freq_dict, bigram_freq_dict, trigram_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugrams, bigrams, trigrams = extract_123grams(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
